{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from models.xswin import XNetSwinTransformer\n",
    "from models.xswin_diffusion import XNetSwinTransformerDiffusion\n",
    "from models.modules import SwinResidualCrossAttention\n",
    "\n",
    "from models.modules.normal.residual_cross_attention import _extract_windows, _unfold_padding_prep, _fold_unpadding_prep\n",
    "\n",
    "from torchinfo import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 8, 5])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([\n",
    "    [1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4],\n",
    "    [1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4],\n",
    "    [1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4],\n",
    "    [1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4],\n",
    "])\n",
    "\n",
    "x = torch.tensor([\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "])\n",
    "\n",
    "# print(torch.cat((x, x), dim=-1))\n",
    "\n",
    "\n",
    "# x = torch.vstack((torch.hstack((x, x*2)), torch.hstack((x*3, x*4))))\n",
    "x = torch.stack((x, x*2, x*3, x*4))\n",
    "x = torch.vstack((torch.hstack((x, x*2)), torch.hstack((x*3, x*4))))\n",
    "\n",
    "\n",
    "# x = x.reshape(2, 2, -1)\n",
    "x = x.unsqueeze(0)\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "x = _extract_windows(x, 4, 4)\n",
    "\n",
    "# print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 12, 24, 16])\n",
      "torch.Size([5, 10, 22, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 22, 16])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_DIM = 16\n",
    "\n",
    "r1 = torch.ones((5, 10, 22, H_DIM))\n",
    "r2 = torch.ones(r1.shape)\n",
    "\n",
    "f1, pinfo = _unfold_padding_prep(r1, 4, 4)\n",
    "print(f1.shape)\n",
    "f2 = _fold_unpadding_prep(f1, pinfo)\n",
    "print(f2.shape)\n",
    "\n",
    "SRCA = SwinResidualCrossAttention(\n",
    "    [4, 4],\n",
    "    H_DIM,\n",
    "    4,\n",
    ")\n",
    "\n",
    "SRCA(r1, r2).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 64, 151, 309]) torch.Size([10, 64])\n",
      "torch.Size([10, 64, 151, 309]) torch.Size([10, 64])\n",
      "torch.Size([10, 64, 151, 309]) torch.Size([10, 64])\n",
      "torch.Size([10, 64, 151, 309]) torch.Size([10, 64])\n",
      "torch.Size([10, 37, 77, 64]) torch.Size([10, 64])\n",
      "torch.Size([10, 37, 77, 64]) torch.Size([10, 64])\n",
      "torch.Size([10, 37, 77, 64]) torch.Size([10, 64])\n",
      "torch.Size([10, 19, 39, 128]) torch.Size([10, 64])\n",
      "torch.Size([10, 19, 39, 128]) torch.Size([10, 64])\n",
      "torch.Size([10, 19, 39, 128]) torch.Size([10, 64])\n",
      "torch.Size([10, 10, 20, 256]) torch.Size([10, 64])\n",
      "torch.Size([10, 10, 20, 256]) torch.Size([10, 64])\n",
      "torch.Size([10, 10, 20, 256]) torch.Size([10, 64])\n",
      "torch.Size([10, 50, 512]) torch.Size([10, 64])\n",
      "torch.Size([10, 50, 512]) torch.Size([10, 64])\n",
      "torch.Size([10, 5, 10, 512]) torch.Size([10, 64])\n",
      "torch.Size([10, 10, 20, 256]) torch.Size([10, 64])\n",
      "torch.Size([10, 10, 20, 256]) torch.Size([10, 64])\n",
      "torch.Size([10, 10, 20, 512]) torch.Size([10, 64])\n",
      "torch.Size([10, 10, 20, 512]) torch.Size([10, 64])\n",
      "torch.Size([10, 10, 20, 512]) torch.Size([10, 64])\n",
      "torch.Size([10, 10, 20, 256]) torch.Size([10, 64])\n",
      "torch.Size([10, 19, 39, 128]) torch.Size([10, 64])\n",
      "torch.Size([10, 19, 39, 128]) torch.Size([10, 64])\n",
      "torch.Size([10, 19, 39, 256]) torch.Size([10, 64])\n",
      "torch.Size([10, 19, 39, 256]) torch.Size([10, 64])\n",
      "torch.Size([10, 19, 39, 256]) torch.Size([10, 64])\n",
      "torch.Size([10, 19, 39, 128]) torch.Size([10, 64])\n",
      "torch.Size([10, 37, 77, 64]) torch.Size([10, 64])\n",
      "torch.Size([10, 37, 77, 64]) torch.Size([10, 64])\n",
      "torch.Size([10, 37, 77, 128]) torch.Size([10, 64])\n",
      "torch.Size([10, 37, 77, 128]) torch.Size([10, 64])\n",
      "torch.Size([10, 37, 77, 128]) torch.Size([10, 64])\n",
      "torch.Size([10, 37, 77, 64]) torch.Size([10, 64])\n",
      "torch.Size([10, 128, 151, 309]) torch.Size([10, 64])\n",
      "torch.Size([10, 64, 151, 309]) torch.Size([10, 64])\n",
      "torch.Size([10, 64, 151, 309]) torch.Size([10, 64])\n",
      "torch.Size([10, 64, 151, 309]) torch.Size([10, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 2.7529e-01,  4.9275e-01,  1.8895e-01,  ...,  2.5477e-01,\n",
       "            3.4588e-01,  4.9383e-01],\n",
       "          [ 4.0597e-01,  3.2908e-01,  4.5913e-01,  ...,  3.9881e-01,\n",
       "            5.7380e-01,  4.9717e-01],\n",
       "          [ 3.7970e-02, -3.4107e-01, -1.8008e-01,  ...,  3.8295e-01,\n",
       "            3.0138e-01,  4.8041e-01],\n",
       "          ...,\n",
       "          [ 2.9419e-01,  3.5661e-01,  2.5188e-01,  ...,  7.9012e-03,\n",
       "            2.6368e-01,  7.3264e-03],\n",
       "          [ 2.7711e-01,  5.0808e-02,  3.4199e-02,  ..., -3.0745e-01,\n",
       "           -7.9126e-03,  3.2583e-01],\n",
       "          [ 5.5517e-01,  4.3856e-01,  8.2591e-02,  ...,  2.4407e-01,\n",
       "            1.6944e-01,  5.0870e-01]],\n",
       "\n",
       "         [[ 4.0396e-02,  2.5708e-01,  4.4687e-02,  ...,  5.0010e-01,\n",
       "            1.9652e-01,  2.9233e-01],\n",
       "          [ 1.5042e-01,  1.9112e-01,  2.3647e-01,  ...,  6.7613e-01,\n",
       "           -7.1661e-02,  6.1299e-01],\n",
       "          [ 7.1949e-02,  4.5296e-01,  4.1781e-01,  ...,  4.7238e-01,\n",
       "            1.4058e-01,  3.8443e-01],\n",
       "          ...,\n",
       "          [ 2.8054e-02,  7.9961e-02, -3.0491e-01,  ..., -7.8901e-02,\n",
       "            1.5570e-01,  5.2415e-01],\n",
       "          [ 1.1650e-01, -4.9833e-03,  9.0299e-02,  ...,  5.5804e-01,\n",
       "            6.8200e-01,  4.8943e-01],\n",
       "          [ 4.5302e-01,  4.0740e-01,  3.1922e-01,  ...,  4.2786e-01,\n",
       "            4.5292e-01,  5.0761e-01]],\n",
       "\n",
       "         [[-2.9305e-01, -4.5993e-02,  2.4063e-03,  ...,  1.1523e-01,\n",
       "           -4.7852e-02, -3.0570e-02],\n",
       "          [-7.6852e-01, -3.7864e-02, -1.4417e-01,  ..., -4.0347e-03,\n",
       "           -2.9835e-01, -4.0849e-01],\n",
       "          [-4.6418e-01, -2.7316e-01,  9.6684e-02,  ..., -1.9796e-01,\n",
       "           -1.8722e-01,  3.2682e-01],\n",
       "          ...,\n",
       "          [-1.3030e-01, -6.3070e-01, -7.3747e-01,  ...,  5.1825e-03,\n",
       "            1.1341e-01,  4.2634e-02],\n",
       "          [-2.2615e-01, -8.1843e-02,  3.3095e-01,  ..., -1.5996e-01,\n",
       "           -1.4533e-01, -2.2592e-01],\n",
       "          [-3.9223e-01, -1.6023e-01, -2.4716e-02,  ..., -2.3634e-02,\n",
       "            1.1209e-01, -2.7692e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.1475e-01,  2.4205e-01,  2.5559e-01,  ...,  8.1383e-02,\n",
       "            3.5037e-01,  4.1054e-01],\n",
       "          [ 3.0061e-01,  8.7242e-01,  2.4963e-02,  ..., -3.0878e-01,\n",
       "           -1.9294e-01, -1.6431e-02],\n",
       "          [ 4.6926e-01,  2.1302e-01,  2.9979e-01,  ...,  7.5070e-01,\n",
       "            5.8649e-02,  4.4269e-01],\n",
       "          ...,\n",
       "          [-2.6558e-02,  3.7894e-01,  4.9790e-01,  ...,  6.1245e-01,\n",
       "           -4.8730e-02,  4.7676e-01],\n",
       "          [ 7.0921e-02,  4.0667e-01, -4.3469e-02,  ..., -1.0055e-01,\n",
       "            6.6665e-03,  6.8216e-01],\n",
       "          [-1.4452e-02,  1.4299e-01,  5.2608e-01,  ...,  3.9185e-01,\n",
       "            5.7844e-01,  3.8983e-01]],\n",
       "\n",
       "         [[-7.4929e-02, -3.7396e-01,  2.5343e-01,  ..., -1.5020e-01,\n",
       "           -1.9499e-01, -2.3549e-01],\n",
       "          [-3.8069e-01, -9.5563e-02,  2.6076e-01,  ...,  5.4865e-01,\n",
       "            2.5429e-01,  6.4200e-02],\n",
       "          [-9.1468e-03,  8.2061e-02, -1.0078e-01,  ..., -9.2576e-02,\n",
       "           -1.1774e-03, -2.6889e-01],\n",
       "          ...,\n",
       "          [ 2.3850e-01,  6.7803e-01, -8.8556e-02,  ..., -1.0128e-01,\n",
       "            6.7155e-02, -6.4622e-01],\n",
       "          [ 3.4384e-01,  9.2209e-02, -5.8699e-01,  ...,  9.9312e-02,\n",
       "           -2.3015e-01,  1.3654e-01],\n",
       "          [-1.2296e-01, -6.0394e-02,  3.6959e-01,  ..., -1.6923e-01,\n",
       "            2.1456e-01,  5.9097e-02]],\n",
       "\n",
       "         [[ 1.2727e-01, -5.0622e-02, -1.0575e-01,  ...,  3.0826e-01,\n",
       "            6.7578e-03, -5.5821e-02],\n",
       "          [ 2.2441e-01,  3.9149e-01,  1.5478e-01,  ...,  3.2304e-01,\n",
       "            1.7946e-01, -1.8087e-01],\n",
       "          [ 6.7169e-01,  5.2623e-01,  7.3507e-01,  ...,  6.1559e-01,\n",
       "            5.8237e-02,  8.6347e-02],\n",
       "          ...,\n",
       "          [-9.8009e-02, -6.6181e-02,  1.0323e-01,  ...,  4.3956e-03,\n",
       "           -6.6386e-03,  1.9937e-01],\n",
       "          [-1.6370e-01,  3.2875e-01,  4.5348e-01,  ..., -6.1960e-03,\n",
       "            3.1328e-01, -6.6223e-02],\n",
       "          [-2.0637e-01,  2.4903e-01, -5.1936e-02,  ..., -5.0662e-02,\n",
       "           -2.9445e-01, -4.0004e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 3.7129e-01,  3.8161e-01,  2.0106e-01,  ...,  3.0781e-02,\n",
       "            4.2401e-01,  3.9936e-01],\n",
       "          [ 1.7955e-01,  2.9129e-01, -1.2181e-01,  ...,  5.2226e-01,\n",
       "            6.2247e-02,  4.9204e-01],\n",
       "          [ 3.6998e-01,  8.9239e-02,  1.7493e-01,  ..., -2.7667e-01,\n",
       "            1.8690e-01,  5.9697e-01],\n",
       "          ...,\n",
       "          [ 4.8805e-01,  3.3893e-01,  3.9521e-01,  ..., -1.2634e-01,\n",
       "            1.8087e-01,  3.2352e-01],\n",
       "          [ 4.0796e-01,  9.0849e-02, -5.2958e-02,  ...,  4.0220e-01,\n",
       "            4.7808e-01,  4.0250e-01],\n",
       "          [ 5.0216e-01,  2.4644e-01,  1.4326e-01,  ...,  2.1547e-01,\n",
       "            3.1122e-01,  3.8240e-01]],\n",
       "\n",
       "         [[ 1.9539e-01,  4.0550e-01,  3.7331e-01,  ...,  4.3923e-01,\n",
       "            4.1144e-02,  2.3587e-01],\n",
       "          [ 4.6936e-01,  1.7567e-01,  5.3289e-01,  ...,  6.0950e-01,\n",
       "            3.9043e-01,  3.9937e-01],\n",
       "          [ 2.2340e-01,  5.7542e-01,  4.5120e-02,  ...,  4.4884e-01,\n",
       "            1.4521e-01,  6.8869e-01],\n",
       "          ...,\n",
       "          [ 1.9041e-01, -8.8526e-02,  1.3531e-01,  ...,  3.2476e-01,\n",
       "            3.5452e-01,  3.5195e-01],\n",
       "          [ 1.6392e-01,  3.9843e-01, -1.9116e-01,  ...,  4.1374e-01,\n",
       "            5.8442e-01,  4.3563e-01],\n",
       "          [ 2.7474e-01,  4.3325e-01,  3.8700e-01,  ...,  2.5915e-01,\n",
       "            2.9944e-01,  7.3319e-01]],\n",
       "\n",
       "         [[-2.9975e-01, -5.2372e-02,  1.4443e-02,  ..., -2.4753e-01,\n",
       "           -2.6174e-01, -1.1891e-01],\n",
       "          [-4.0662e-01, -9.8734e-02, -1.0296e-01,  ..., -3.4680e-01,\n",
       "            4.8839e-01, -3.7480e-02],\n",
       "          [-7.4007e-02, -5.8913e-01, -6.1583e-01,  ..., -6.0158e-02,\n",
       "           -2.9977e-01, -1.8994e-01],\n",
       "          ...,\n",
       "          [-2.8050e-01, -5.9126e-01, -1.3560e-01,  ...,  6.4967e-02,\n",
       "            3.0201e-01, -1.3776e-01],\n",
       "          [-3.0873e-01, -6.3653e-02,  7.7907e-02,  ...,  6.8943e-02,\n",
       "            8.9567e-02, -1.2253e-01],\n",
       "          [-2.2211e-01,  3.2921e-02, -5.8512e-02,  ...,  5.3685e-02,\n",
       "           -7.9181e-02, -9.4818e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.9470e-01,  3.6123e-01,  2.3713e-01,  ...,  1.2249e-01,\n",
       "            5.6217e-02,  2.6866e-01],\n",
       "          [ 4.1135e-01, -4.1528e-02,  1.4384e-01,  ...,  3.6918e-01,\n",
       "            4.6351e-01,  7.6411e-02],\n",
       "          [ 3.7220e-01,  3.6470e-02, -1.4637e-01,  ...,  1.6336e-01,\n",
       "           -1.7630e-02,  2.5672e-01],\n",
       "          ...,\n",
       "          [ 4.8217e-01,  3.2185e-01,  3.7683e-01,  ...,  3.3712e-01,\n",
       "            6.8040e-02,  5.3901e-02],\n",
       "          [ 2.0007e-01,  4.1578e-01,  6.3864e-02,  ..., -7.3239e-02,\n",
       "            2.6498e-01,  6.0665e-01],\n",
       "          [ 1.8440e-02,  3.7021e-01,  3.7816e-01,  ...,  3.0928e-01,\n",
       "            1.6516e-01,  3.5574e-01]],\n",
       "\n",
       "         [[-9.5498e-03, -1.4666e-02, -1.5816e-03,  ...,  4.2175e-01,\n",
       "            6.3412e-02, -1.4428e-01],\n",
       "          [-4.1876e-01,  5.0916e-01, -1.4665e-01,  ..., -1.0112e-01,\n",
       "            1.6130e-01, -1.9167e-01],\n",
       "          [ 2.2693e-04,  6.5201e-01, -1.7084e-01,  ...,  1.8060e-02,\n",
       "            2.7366e-01, -2.3460e-01],\n",
       "          ...,\n",
       "          [-2.6922e-01,  8.0999e-03, -6.4730e-02,  ...,  8.4704e-02,\n",
       "            4.4176e-01, -1.2306e-02],\n",
       "          [ 6.5046e-02,  2.5020e-01, -6.7534e-02,  ..., -2.5011e-01,\n",
       "           -5.6787e-02, -4.6162e-02],\n",
       "          [-2.6495e-02,  5.6163e-02, -5.9967e-02,  ...,  2.9300e-01,\n",
       "            2.9644e-02,  2.0805e-02]],\n",
       "\n",
       "         [[-8.5715e-02, -1.2523e-01,  1.2894e-01,  ..., -2.7964e-01,\n",
       "            6.7992e-02,  9.4154e-02],\n",
       "          [ 1.8087e-01, -7.7844e-02,  6.9641e-01,  ...,  4.1091e-01,\n",
       "           -6.7121e-02,  1.7089e-01],\n",
       "          [ 2.8441e-01,  2.5474e-01,  2.5348e-01,  ...,  4.0561e-01,\n",
       "            8.7496e-02,  2.9659e-01],\n",
       "          ...,\n",
       "          [ 2.6957e-02, -1.0681e-01,  1.5191e-01,  ...,  1.3262e-01,\n",
       "            1.7848e-01, -8.8435e-02],\n",
       "          [ 3.1959e-02,  2.6842e-01,  2.5846e-01,  ...,  2.0443e-02,\n",
       "            3.6466e-02, -2.1052e-02],\n",
       "          [-1.7548e-01, -8.1060e-02,  1.5063e-01,  ...,  7.0443e-02,\n",
       "           -2.2036e-01, -1.9982e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 3.5371e-01,  3.1995e-01, -6.0043e-02,  ...,  2.3002e-01,\n",
       "            4.8267e-02,  3.0469e-01],\n",
       "          [ 3.1577e-02,  3.8261e-01, -5.9563e-02,  ...,  6.6726e-01,\n",
       "            4.5802e-01,  4.6738e-01],\n",
       "          [ 1.0509e-01,  1.3922e-01,  3.4598e-01,  ..., -9.7802e-02,\n",
       "            5.3601e-01,  3.0145e-01],\n",
       "          ...,\n",
       "          [ 1.8464e-01, -1.3939e-01,  5.1747e-01,  ..., -3.0208e-02,\n",
       "            7.0459e-02,  2.8807e-01],\n",
       "          [ 3.3826e-01,  3.2171e-01,  1.6073e-01,  ...,  1.1741e-01,\n",
       "            3.2561e-01,  5.1215e-01],\n",
       "          [ 4.5354e-01,  3.5414e-01,  1.3231e-02,  ...,  1.6576e-01,\n",
       "            2.6074e-01,  5.4432e-01]],\n",
       "\n",
       "         [[ 1.8036e-01,  2.4610e-01,  1.4790e-01,  ...,  2.9475e-01,\n",
       "            1.4912e-01,  3.6670e-01],\n",
       "          [ 4.9215e-01,  1.3510e-01,  7.7731e-01,  ...,  5.0029e-01,\n",
       "            1.6839e-01,  5.1111e-01],\n",
       "          [-3.2885e-01,  6.6972e-01,  3.8209e-01,  ...,  4.1835e-01,\n",
       "            1.5781e-01,  2.2486e-01],\n",
       "          ...,\n",
       "          [-4.6036e-03,  2.8315e-01,  1.7799e-01,  ...,  1.4067e-01,\n",
       "            6.9735e-01,  4.8428e-01],\n",
       "          [ 2.3367e-01,  2.3214e-01,  5.2484e-01,  ...,  3.5311e-01,\n",
       "            2.5160e-01,  5.6216e-01],\n",
       "          [ 4.4000e-01,  5.8411e-01,  4.7684e-01,  ...,  4.0823e-01,\n",
       "            4.5260e-01,  5.2288e-01]],\n",
       "\n",
       "         [[-3.3332e-01, -1.1123e-01, -2.2960e-01,  ..., -2.1485e-01,\n",
       "           -2.7482e-01,  1.3416e-01],\n",
       "          [-3.8143e-01, -2.3356e-01,  1.9794e-01,  ..., -3.4634e-01,\n",
       "            1.1570e-01, -3.8856e-01],\n",
       "          [-1.5844e-01, -6.8487e-01, -5.1304e-01,  ..., -3.1895e-01,\n",
       "            1.6187e-02,  1.4397e-01],\n",
       "          ...,\n",
       "          [-1.6081e-02,  1.4590e-01, -4.7277e-01,  ..., -2.1169e-02,\n",
       "            1.2293e-01, -1.8434e-01],\n",
       "          [-9.4603e-02, -1.2241e-01, -5.0740e-01,  ...,  2.2803e-01,\n",
       "            5.2215e-02,  1.1765e-01],\n",
       "          [-2.6839e-01, -1.6347e-01,  7.4429e-02,  ...,  4.1601e-02,\n",
       "            1.1588e-01, -9.4185e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.0836e-01,  4.6713e-01,  6.8098e-01,  ...,  6.9644e-02,\n",
       "            1.8376e-01,  5.1943e-01],\n",
       "          [ 5.1805e-01,  3.0143e-01, -1.6792e-01,  ..., -1.5423e-01,\n",
       "            1.2263e-01,  3.4479e-01],\n",
       "          [ 3.2118e-01, -6.1672e-02,  2.0519e-01,  ...,  1.6234e-01,\n",
       "            3.0613e-02,  1.9242e-01],\n",
       "          ...,\n",
       "          [ 3.1804e-01,  2.2458e-01,  2.3912e-01,  ...,  1.2163e-01,\n",
       "            1.9428e-01,  4.4498e-01],\n",
       "          [ 1.2128e-01,  1.9750e-02,  2.0799e-01,  ...,  2.1251e-01,\n",
       "            3.2657e-01,  4.4416e-01],\n",
       "          [-1.9233e-02,  1.1984e-01,  4.2412e-01,  ...,  2.8390e-01,\n",
       "            1.5091e-01,  3.4607e-01]],\n",
       "\n",
       "         [[ 1.3125e-01, -1.4190e-01,  6.6853e-02,  ..., -3.1676e-01,\n",
       "           -2.7853e-02, -9.6877e-02],\n",
       "          [-1.4264e-01,  1.7539e-01, -1.8044e-01,  ...,  1.3972e-01,\n",
       "           -2.2566e-01, -2.0941e-01],\n",
       "          [-2.5650e-01,  1.0080e-01, -1.6232e-01,  ..., -3.4255e-01,\n",
       "           -3.4698e-02, -3.1092e-01],\n",
       "          ...,\n",
       "          [ 1.1680e-01,  1.1257e-01, -3.5069e-01,  ..., -5.7549e-03,\n",
       "           -1.6469e-02, -2.1376e-01],\n",
       "          [ 1.7861e-03, -2.8555e-02,  2.8212e-01,  ...,  2.0206e-01,\n",
       "           -2.1483e-02, -2.0136e-01],\n",
       "          [-6.6850e-02, -5.9525e-02,  3.4837e-01,  ...,  2.4778e-01,\n",
       "            1.6587e-01, -1.1621e-01]],\n",
       "\n",
       "         [[-1.0067e-01, -3.2238e-02,  1.0271e-01,  ..., -2.9279e-02,\n",
       "            1.5446e-01, -2.0173e-01],\n",
       "          [ 4.4973e-01,  2.7564e-01,  2.4369e-01,  ...,  2.4827e-01,\n",
       "            3.8035e-01, -1.0450e-02],\n",
       "          [ 2.4021e-01,  4.0882e-01,  2.4899e-01,  ...,  6.1876e-01,\n",
       "            7.2952e-03,  1.0232e-01],\n",
       "          ...,\n",
       "          [-1.2596e-01,  2.3883e-01, -2.5353e-01,  ...,  1.4333e-01,\n",
       "            4.7627e-01,  3.6009e-02],\n",
       "          [ 1.2814e-01,  2.0660e-01,  1.8178e-01,  ..., -1.2177e-02,\n",
       "           -4.1423e-02,  3.9540e-03],\n",
       "          [-2.0513e-01,  2.4803e-01,  3.7893e-02,  ..., -1.6456e-01,\n",
       "           -1.8771e-02, -2.8276e-01]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 5.3697e-01,  3.7611e-01, -3.5734e-02,  ...,  3.3531e-01,\n",
       "            2.8097e-01,  4.4321e-01],\n",
       "          [ 1.8150e-01, -3.0126e-01, -8.9164e-02,  ...,  4.5972e-01,\n",
       "            5.3132e-02,  3.1389e-01],\n",
       "          [ 8.8895e-02, -2.1424e-01,  5.2605e-01,  ..., -1.2388e-01,\n",
       "            6.5122e-01,  3.0365e-01],\n",
       "          ...,\n",
       "          [ 5.0036e-01,  2.3761e-01,  2.3156e-01,  ...,  1.0473e-01,\n",
       "           -2.2983e-01,  4.2724e-01],\n",
       "          [ 5.6303e-01, -2.5943e-02,  1.1755e-02,  ..., -3.8150e-01,\n",
       "           -5.6844e-03,  3.4203e-01],\n",
       "          [ 3.8475e-01,  1.5295e-01, -1.9235e-01,  ...,  3.4536e-01,\n",
       "            3.2738e-01,  5.2269e-01]],\n",
       "\n",
       "         [[ 1.1378e-01,  2.1788e-01,  1.5560e-01,  ...,  3.6896e-01,\n",
       "            7.0392e-02,  9.9389e-02],\n",
       "          [ 5.3430e-01,  4.1070e-01,  6.1024e-01,  ..., -3.7975e-02,\n",
       "            4.6356e-01,  6.7977e-01],\n",
       "          [ 6.3526e-02, -1.7120e-02,  2.8182e-01,  ...,  7.2174e-01,\n",
       "            2.1620e-01,  1.7198e-01],\n",
       "          ...,\n",
       "          [ 2.1929e-02,  3.0408e-01, -1.7774e-01,  ..., -1.4562e-01,\n",
       "            3.5595e-01,  2.5736e-01],\n",
       "          [ 1.6425e-01,  5.7853e-01, -6.8215e-02,  ...,  1.0954e-01,\n",
       "            1.9419e-01,  4.5648e-01],\n",
       "          [ 3.2604e-01,  2.2249e-01,  6.5930e-01,  ...,  3.1035e-01,\n",
       "            6.8201e-01,  5.7601e-01]],\n",
       "\n",
       "         [[-3.7538e-01,  2.3059e-01,  5.2498e-03,  ..., -7.7354e-02,\n",
       "           -3.7614e-01,  8.3910e-03],\n",
       "          [-2.4338e-01, -2.3644e-02,  4.2760e-02,  ..., -1.8323e-01,\n",
       "            1.8655e-01,  5.7764e-02],\n",
       "          [-2.4371e-01, -5.4305e-01,  8.0758e-02,  ...,  1.0462e-01,\n",
       "           -2.9780e-01,  9.9902e-03],\n",
       "          ...,\n",
       "          [-3.2582e-01, -9.0757e-01, -5.6166e-01,  ...,  1.9181e-01,\n",
       "            3.0428e-01, -2.2771e-01],\n",
       "          [-1.3422e-01, -1.0371e-01,  1.6863e-01,  ...,  4.2961e-01,\n",
       "           -1.2496e-01,  1.1746e-03],\n",
       "          [-2.6947e-01, -3.9671e-01, -6.9952e-02,  ..., -3.6942e-02,\n",
       "           -2.3575e-01, -2.0989e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.0494e-01,  3.1632e-01,  5.0053e-01,  ...,  2.3121e-01,\n",
       "            2.7813e-01,  4.0442e-01],\n",
       "          [ 3.6716e-01,  4.5670e-01, -4.2959e-01,  ..., -5.3636e-03,\n",
       "           -1.0915e-02,  4.0185e-01],\n",
       "          [ 2.4919e-01,  7.5678e-02, -2.5820e-01,  ...,  1.6276e-01,\n",
       "           -1.5354e-01,  1.6267e-01],\n",
       "          ...,\n",
       "          [-1.3720e-01,  3.4972e-01,  4.3100e-01,  ...,  2.2432e-01,\n",
       "            1.8091e-01,  1.7853e-01],\n",
       "          [ 1.5675e-01,  2.7569e-01, -4.6807e-02,  ..., -2.5529e-01,\n",
       "            3.2663e-01,  4.9346e-01],\n",
       "          [ 1.2125e-01, -1.4992e-02,  7.3185e-01,  ...,  7.3885e-02,\n",
       "            1.1469e-01,  4.0680e-01]],\n",
       "\n",
       "         [[-6.3452e-02, -2.1850e-01, -8.8318e-03,  ..., -4.5714e-02,\n",
       "            2.1515e-02, -1.9146e-01],\n",
       "          [-2.3031e-01, -7.5930e-02, -3.2921e-01,  ...,  1.7664e-01,\n",
       "            9.6293e-02, -2.1060e-01],\n",
       "          [ 8.6191e-02, -2.5271e-01, -5.1812e-01,  ..., -4.2781e-01,\n",
       "            2.5473e-01, -1.0148e-01],\n",
       "          ...,\n",
       "          [-7.5526e-02,  2.7662e-01, -4.5466e-01,  ..., -5.1397e-01,\n",
       "           -3.6503e-02,  2.9775e-01],\n",
       "          [ 1.8876e-01,  2.6268e-01, -1.4224e-01,  ...,  2.2877e-01,\n",
       "            9.4425e-02,  4.1564e-01],\n",
       "          [-1.3060e-03, -1.2115e-01,  1.8593e-01,  ...,  1.4181e-02,\n",
       "           -2.1078e-01,  5.9796e-02]],\n",
       "\n",
       "         [[-1.7073e-01,  6.0779e-02,  3.2357e-01,  ...,  1.8235e-01,\n",
       "           -5.9428e-02, -2.8241e-02],\n",
       "          [ 7.4021e-02,  3.3093e-01,  2.2463e-01,  ...,  1.5616e-01,\n",
       "            2.1567e-01, -1.0094e-01],\n",
       "          [ 5.4195e-01,  5.7944e-01,  1.7521e-01,  ...,  8.6771e-01,\n",
       "           -1.1116e-01,  1.5212e-01],\n",
       "          ...,\n",
       "          [ 1.2141e-01,  8.3453e-01,  5.5917e-01,  ...,  2.7639e-01,\n",
       "            2.0280e-01, -3.0539e-01],\n",
       "          [-1.1863e-01,  3.9002e-02,  3.0324e-02,  ..., -2.4775e-01,\n",
       "            2.9849e-01,  8.6546e-03],\n",
       "          [-5.9825e-02,  1.1837e-01, -1.4799e-02,  ...,  4.4447e-02,\n",
       "           -1.9595e-01, -3.9805e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 3.4829e-01,  2.8271e-01, -2.8021e-01,  ..., -2.5924e-01,\n",
       "            3.5772e-01,  3.2122e-01],\n",
       "          [ 4.8591e-01,  4.5173e-02,  1.4460e-03,  ...,  4.1226e-01,\n",
       "            4.7864e-01,  6.1811e-01],\n",
       "          [ 1.7760e-01, -3.2761e-01,  2.9836e-01,  ..., -1.5278e-01,\n",
       "            5.6412e-01,  2.6152e-01],\n",
       "          ...,\n",
       "          [ 5.0047e-01,  1.2409e-01,  3.8740e-01,  ..., -3.6003e-01,\n",
       "            1.7526e-02,  4.6346e-01],\n",
       "          [ 5.2326e-01, -1.2083e-02,  1.1531e-02,  ..., -2.0763e-02,\n",
       "           -2.8276e-02,  1.9373e-01],\n",
       "          [ 4.6955e-01,  3.8258e-01, -3.1478e-01,  ...,  7.4988e-02,\n",
       "            2.4337e-01,  4.5182e-01]],\n",
       "\n",
       "         [[ 6.1975e-02,  2.5951e-01,  2.1227e-01,  ...,  6.6817e-01,\n",
       "            8.3102e-02,  1.2409e-01],\n",
       "          [ 4.7302e-01,  6.2359e-01,  9.8800e-01,  ...,  4.2426e-01,\n",
       "            1.5068e-01,  8.0896e-01],\n",
       "          [ 2.5241e-01,  7.2518e-01, -2.3749e-02,  ...,  6.2922e-01,\n",
       "           -2.6741e-01,  2.6529e-01],\n",
       "          ...,\n",
       "          [ 6.5814e-02,  4.5564e-02,  4.3168e-01,  ...,  5.7837e-02,\n",
       "            2.6962e-01,  3.2949e-01],\n",
       "          [ 1.9431e-01,  3.2266e-01,  2.9161e-01,  ...,  7.1050e-01,\n",
       "            4.1693e-02,  3.8379e-01],\n",
       "          [ 5.1627e-01,  2.7032e-01,  4.7855e-01,  ...,  4.1027e-01,\n",
       "            5.7477e-01,  4.7904e-01]],\n",
       "\n",
       "         [[-3.8032e-01,  4.5009e-02,  2.2610e-01,  ...,  6.0925e-01,\n",
       "           -3.9806e-01, -6.6726e-02],\n",
       "          [-4.0769e-01, -4.8549e-02, -9.1449e-02,  ..., -1.1428e-01,\n",
       "            1.7127e-01, -4.6367e-01],\n",
       "          [-1.0780e-01, -4.3337e-01, -1.0579e-01,  ...,  1.0026e-01,\n",
       "           -2.3992e-01,  1.9405e-01],\n",
       "          ...,\n",
       "          [-3.8918e-01, -7.9042e-01, -1.5286e-01,  ..., -2.7023e-02,\n",
       "            1.5773e-03, -3.3636e-01],\n",
       "          [-2.6826e-01, -3.6394e-01,  3.2824e-01,  ...,  3.7740e-01,\n",
       "           -2.1867e-01, -1.0504e-01],\n",
       "          [-2.9858e-01, -1.9680e-01, -1.0100e-01,  ..., -1.0289e-02,\n",
       "            4.3526e-01, -2.8223e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.3896e-01,  5.3181e-01,  1.6270e-01,  ...,  1.2831e-01,\n",
       "            2.5204e-01,  4.6076e-01],\n",
       "          [ 2.1141e-01,  5.6264e-01,  5.3949e-02,  ..., -4.5237e-02,\n",
       "            3.5923e-03,  9.5511e-02],\n",
       "          [ 3.0242e-01,  5.2617e-02, -4.8739e-01,  ...,  3.9069e-01,\n",
       "           -1.0009e-01,  4.3283e-01],\n",
       "          ...,\n",
       "          [ 1.5305e-01,  2.6569e-01,  1.3997e-01,  ...,  3.3004e-01,\n",
       "            4.4242e-01,  2.8550e-01],\n",
       "          [-1.2827e-01,  4.5039e-01,  1.0268e-01,  ...,  4.7424e-02,\n",
       "            2.6371e-01,  3.9163e-01],\n",
       "          [ 1.7307e-01, -3.0857e-02,  5.6903e-01,  ...,  2.1728e-01,\n",
       "            2.3902e-01,  4.0098e-01]],\n",
       "\n",
       "         [[ 8.9969e-02, -3.9790e-01,  1.8300e-01,  ...,  1.6766e-01,\n",
       "           -1.0808e-01, -1.8613e-01],\n",
       "          [-5.3298e-01, -2.1514e-01, -1.1943e-01,  ...,  1.0462e-01,\n",
       "           -2.5412e-01, -5.0272e-02],\n",
       "          [-1.8857e-01,  3.3250e-01, -1.0001e-01,  ..., -2.4688e-01,\n",
       "            3.6302e-01, -1.4204e-01],\n",
       "          ...,\n",
       "          [-6.0893e-02,  2.9464e-01, -4.5730e-01,  ...,  1.7600e-01,\n",
       "            1.3005e-01,  1.8352e-01],\n",
       "          [ 1.6193e-01,  2.3469e-01,  3.0550e-01,  ...,  1.0158e-01,\n",
       "            3.9371e-01, -6.5066e-02],\n",
       "          [-1.4793e-01, -3.1199e-01,  4.0659e-01,  ..., -1.5139e-02,\n",
       "           -9.9016e-02, -4.9247e-02]],\n",
       "\n",
       "         [[-1.7396e-01,  2.1317e-01,  3.2509e-01,  ...,  2.5065e-01,\n",
       "            1.1966e-01, -1.8735e-02],\n",
       "          [ 2.9130e-01,  5.7719e-02,  5.8289e-01,  ...,  3.5134e-01,\n",
       "            5.1708e-01,  2.6307e-01],\n",
       "          [ 2.8037e-01,  5.3189e-01,  2.1931e-01,  ...,  9.7346e-01,\n",
       "            2.6047e-01,  3.8316e-01],\n",
       "          ...,\n",
       "          [ 1.2675e-01, -9.8114e-02,  5.2388e-01,  ...,  3.5497e-01,\n",
       "            9.4375e-02, -3.5690e-02],\n",
       "          [-3.6433e-02,  1.2257e-02,  1.9867e-01,  ..., -1.1500e-01,\n",
       "            7.9120e-02,  1.8244e-01],\n",
       "          [-8.9122e-02,  2.9837e-01, -1.9307e-02,  ..., -1.8948e-01,\n",
       "            5.5102e-02, -2.7831e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 4.4918e-01,  3.5382e-01,  1.1235e-01,  ..., -9.0791e-02,\n",
       "            4.6982e-01,  4.1176e-01],\n",
       "          [ 1.1196e-01,  4.7059e-02,  2.2337e-01,  ...,  3.8892e-01,\n",
       "            3.7104e-01,  6.2426e-01],\n",
       "          [ 3.4629e-01, -1.8978e-01,  5.7891e-01,  ..., -3.1269e-01,\n",
       "            6.9417e-02,  4.9440e-01],\n",
       "          ...,\n",
       "          [ 2.5542e-01, -1.0530e-01,  3.9624e-01,  ..., -6.0835e-02,\n",
       "           -2.4320e-01,  6.8378e-02],\n",
       "          [ 4.3363e-01,  2.1008e-01,  5.0444e-01,  ..., -3.6079e-02,\n",
       "           -5.0876e-02,  2.1024e-01],\n",
       "          [ 3.1470e-01,  3.4576e-01,  3.5308e-01,  ...,  2.6356e-01,\n",
       "            4.3282e-01,  2.7404e-01]],\n",
       "\n",
       "         [[ 2.6969e-01,  4.1950e-01,  3.5657e-01,  ...,  4.7465e-01,\n",
       "            6.7417e-02,  1.2267e-01],\n",
       "          [ 1.1261e-01,  3.9158e-01,  6.7888e-01,  ...,  3.3270e-01,\n",
       "            4.3837e-01,  3.0589e-01],\n",
       "          [-2.4306e-01,  4.0330e-01,  2.4553e-01,  ...,  6.7247e-01,\n",
       "           -1.9662e-01,  3.0833e-01],\n",
       "          ...,\n",
       "          [ 2.2179e-01, -3.5381e-02,  2.1908e-01,  ...,  1.2810e-01,\n",
       "            4.3548e-01,  4.1930e-01],\n",
       "          [ 2.1282e-01,  2.8687e-01,  1.5945e-01,  ...,  4.5789e-01,\n",
       "            4.5352e-01,  5.2855e-01],\n",
       "          [ 5.1605e-01,  4.3010e-01,  2.1792e-01,  ...,  3.0540e-01,\n",
       "            5.4262e-01,  6.3044e-01]],\n",
       "\n",
       "         [[-2.9237e-01, -6.4742e-02, -2.0722e-01,  ...,  1.1241e-02,\n",
       "           -3.6384e-01, -2.0634e-02],\n",
       "          [-3.9366e-01,  2.2840e-02,  5.1090e-03,  ..., -1.7695e-01,\n",
       "            2.6761e-01, -1.3562e-01],\n",
       "          [-2.8691e-01, -4.5427e-01, -4.8244e-01,  ...,  6.2200e-02,\n",
       "           -5.9666e-01, -2.1237e-02],\n",
       "          ...,\n",
       "          [-3.6366e-01,  2.4697e-01, -2.6917e-01,  ..., -4.7734e-01,\n",
       "            3.5215e-01, -9.7854e-02],\n",
       "          [-1.6665e-01,  1.0989e-01, -2.8253e-01,  ..., -7.2392e-02,\n",
       "            4.5696e-01, -2.3483e-01],\n",
       "          [-2.3419e-01,  2.1035e-04, -1.4376e-01,  ..., -1.2955e-01,\n",
       "           -5.3230e-02, -2.3399e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.7724e-01,  4.5880e-01,  2.3791e-01,  ...,  5.6985e-01,\n",
       "            3.4305e-01,  3.5616e-01],\n",
       "          [ 1.7346e-01,  3.8042e-01, -1.8884e-02,  ..., -1.0795e-01,\n",
       "            1.6344e-01,  4.0239e-01],\n",
       "          [ 9.4263e-02, -9.0362e-02,  2.2511e-01,  ...,  1.1547e-01,\n",
       "           -1.5414e-01,  1.8880e-01],\n",
       "          ...,\n",
       "          [ 2.8278e-01,  4.6816e-01,  3.2056e-01,  ...,  2.0729e-01,\n",
       "            5.8595e-01,  2.7592e-01],\n",
       "          [-2.0606e-02,  1.9285e-01,  3.6992e-01,  ..., -4.7223e-01,\n",
       "            1.7225e-01,  4.3174e-01],\n",
       "          [ 1.3064e-01,  2.5528e-01,  2.3052e-01,  ..., -3.6840e-02,\n",
       "            1.9400e-01,  3.7117e-01]],\n",
       "\n",
       "         [[ 9.4992e-04, -1.6556e-01, -2.0333e-01,  ..., -1.8820e-01,\n",
       "            1.6715e-02, -1.0493e-01],\n",
       "          [-3.6578e-01, -2.2704e-02, -9.3411e-02,  ..., -2.4465e-03,\n",
       "           -3.7398e-02, -1.4071e-02],\n",
       "          [-2.4596e-01,  1.0538e-01, -1.9922e-01,  ...,  7.1707e-02,\n",
       "            5.7981e-01, -1.6277e-01],\n",
       "          ...,\n",
       "          [-2.2688e-01, -9.3381e-02, -9.2959e-02,  ..., -2.8783e-01,\n",
       "            1.7756e-01, -4.3330e-02],\n",
       "          [-3.3006e-02,  2.7438e-01, -3.0902e-01,  ...,  3.4545e-01,\n",
       "            9.8182e-02,  1.9195e-02],\n",
       "          [ 1.1450e-01,  3.2432e-01,  4.5102e-01,  ..., -2.6897e-02,\n",
       "           -4.6076e-03, -4.2530e-02]],\n",
       "\n",
       "         [[-1.2407e-01, -8.9322e-02,  3.2502e-01,  ...,  2.0320e-01,\n",
       "            1.3349e-01, -1.8137e-01],\n",
       "          [ 3.7007e-01,  6.2506e-02,  3.9440e-01,  ...,  3.1162e-01,\n",
       "            3.3642e-02, -8.0897e-02],\n",
       "          [ 2.3879e-01,  2.2404e-01,  4.1261e-01,  ...,  5.5378e-01,\n",
       "           -1.3188e-02,  2.8131e-01],\n",
       "          ...,\n",
       "          [ 3.8988e-01,  1.9710e-01,  2.0474e-01,  ...,  1.1654e-01,\n",
       "            6.0794e-01,  3.3342e-01],\n",
       "          [-1.6050e-02,  1.0950e-01,  3.6847e-01,  ..., -9.7166e-02,\n",
       "            1.2962e-01, -7.4075e-02],\n",
       "          [-8.0982e-02,  1.2849e-01, -3.8707e-01,  ..., -1.1407e-01,\n",
       "           -4.0530e-02, -1.4203e-01]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_size = [4, 4]\n",
    "embed_dim = 64\n",
    "depths = [1, 1, 1]\n",
    "num_heads = [4, 8, 16]\n",
    "window_size = [4, 4]\n",
    "num_classes = 10\n",
    "\n",
    "IMG_H, IMG_W = 151, 309\n",
    "LATENT_DIM = 64\n",
    "B = 10\n",
    "\n",
    "global_stages = 1\n",
    "input_size = [IMG_H, IMG_W]\n",
    "final_downsample = True\n",
    "residual_cross_attention = True\n",
    "class_dropout = 0.1\n",
    "\n",
    "diffusion = XNetSwinTransformerDiffusion(patch_size, embed_dim, depths, \n",
    "                           num_heads, window_size, num_classes=num_classes,\n",
    "                           global_stages=global_stages, input_size=input_size,\n",
    "                           final_downsample=final_downsample, residual_cross_attention=residual_cross_attention,\n",
    "                           class_dropout_prob=class_dropout, latent_dimensions=LATENT_DIM,\n",
    "                           )\n",
    "\n",
    "x = torch.randn((B, LATENT_DIM, IMG_H, IMG_W))\n",
    "t = torch.arange(0, B) # (B, )\n",
    "y = torch.arange(0, B) # (B, )\n",
    "\n",
    "diffusion(x, t, y).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 151, 309])\n",
      "torch.Size([5, 151, 309])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchinfo/torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  action_fn=lambda data: sys.getsizeof(data.storage()),\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return super().__sizeof__() + self.nbytes()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "XNetSwinTransformer                                     [1, 151, 309]             25,600\n",
       "├─ConvolutionTriplet: 1-1                               [1, 64, 151, 309]         --\n",
       "│    └─Sequential: 2-1                                  [1, 64, 151, 309]         --\n",
       "│    │    └─Conv2d: 3-1                                 [1, 64, 151, 309]         1,792\n",
       "│    │    └─BatchNorm2d: 3-2                            [1, 64, 151, 309]         128\n",
       "│    │    └─LeakyReLU: 3-3                              [1, 64, 151, 309]         --\n",
       "│    │    └─Conv2d: 3-4                                 [1, 64, 151, 309]         36,928\n",
       "│    │    └─BatchNorm2d: 3-5                            [1, 64, 151, 309]         128\n",
       "│    │    └─LeakyReLU: 3-6                              [1, 64, 151, 309]         --\n",
       "│    │    └─Conv2d: 3-7                                 [1, 64, 151, 309]         36,928\n",
       "│    │    └─BatchNorm2d: 3-8                            [1, 64, 151, 309]         128\n",
       "│    │    └─LeakyReLU: 3-9                              [1, 64, 151, 309]         --\n",
       "├─Patching: 1-2                                         [1, 37, 77, 64]           --\n",
       "│    └─Sequential: 2-2                                  [1, 37, 77, 64]           --\n",
       "│    │    └─Conv2d: 3-10                                [1, 64, 37, 77]           65,600\n",
       "│    │    └─Permute: 3-11                               [1, 37, 77, 64]           --\n",
       "│    │    └─LayerNorm: 3-12                             [1, 37, 77, 64]           128\n",
       "├─ModuleList: 1-3                                       --                        --\n",
       "│    └─Sequential: 2-3                                  [1, 37, 77, 64]           --\n",
       "│    │    └─SwinTransformerBlockV2: 3-13                [1, 37, 77, 64]           53,572\n",
       "│    └─PatchMergingV2: 2-4                              [1, 19, 39, 128]          --\n",
       "│    │    └─Linear: 3-14                                [1, 19, 39, 128]          32,768\n",
       "│    │    └─LayerNorm: 3-15                             [1, 19, 39, 128]          256\n",
       "│    └─Sequential: 2-5                                  [1, 19, 39, 128]          --\n",
       "│    │    └─SwinTransformerBlockV2: 3-16                [1, 19, 39, 128]          203,912\n",
       "│    └─PatchMergingV2: 2-6                              [1, 10, 20, 256]          --\n",
       "│    │    └─Linear: 3-17                                [1, 10, 20, 256]          131,072\n",
       "│    │    └─LayerNorm: 3-18                             [1, 10, 20, 256]          512\n",
       "│    └─Sequential: 2-7                                  [1, 10, 20, 256]          --\n",
       "│    │    └─SwinTransformerBlockV2: 3-19                [1, 10, 20, 256]          799,504\n",
       "│    └─PatchMergingV2: 2-8                              [1, 5, 10, 512]           --\n",
       "│    │    └─Linear: 3-20                                [1, 5, 10, 512]           524,288\n",
       "│    │    └─LayerNorm: 3-21                             [1, 5, 10, 512]           1,024\n",
       "├─Sequential: 1-4                                       [1, 50, 512]              --\n",
       "│    └─ViTEncoderBlock: 2-9                             [1, 50, 512]              --\n",
       "│    │    └─LayerNorm: 3-22                             [1, 50, 512]              1,024\n",
       "│    │    └─MultiheadAttention: 3-23                    [1, 50, 512]              1,050,624\n",
       "│    │    └─Dropout: 3-24                               [1, 50, 512]              --\n",
       "│    │    └─LayerNorm: 3-25                             [1, 50, 512]              1,024\n",
       "│    │    └─MLPBlock: 3-26                              [1, 50, 512]              2,099,712\n",
       "├─ModuleList: 1-5                                       --                        --\n",
       "│    └─PatchExpandingV2: 2-10                           [1, 10, 20, 256]          --\n",
       "│    │    └─Linear: 3-27                                [1, 5, 10, 1024]          524,288\n",
       "│    │    └─LayerNorm: 3-28                             [1, 5, 10, 1024]          2,048\n",
       "│    └─SwinResidualCrossAttention: 2-11                 [1, 10, 20, 256]          --\n",
       "│    │    └─MultiheadAttention: 3-29                    [18, 16, 256]             263,168\n",
       "│    │    └─LayerNorm: 3-30                             [1, 10, 20, 256]          512\n",
       "│    └─Sequential: 2-12                                 [1, 10, 20, 256]          --\n",
       "│    │    └─SwinTransformerBlockV2: 3-31                [1, 10, 20, 512]          3,170,336\n",
       "│    │    └─PointwiseConvolution: 3-32                  [1, 10, 20, 256]          131,328\n",
       "│    └─PatchExpandingV2: 2-13                           [1, 19, 39, 128]          --\n",
       "│    │    └─Linear: 3-33                                [1, 10, 20, 512]          131,072\n",
       "│    │    └─LayerNorm: 3-34                             [1, 10, 20, 512]          1,024\n",
       "│    └─SwinResidualCrossAttention: 2-14                 [1, 19, 39, 128]          --\n",
       "│    │    └─MultiheadAttention: 3-35                    [50, 16, 128]             66,048\n",
       "│    │    └─LayerNorm: 3-36                             [1, 19, 39, 128]          256\n",
       "│    └─Sequential: 2-15                                 [1, 19, 39, 128]          --\n",
       "│    │    └─SwinTransformerBlockV2: 3-37                [1, 19, 39, 256]          799,504\n",
       "│    │    └─PointwiseConvolution: 3-38                  [1, 19, 39, 128]          32,896\n",
       "│    └─PatchExpandingV2: 2-16                           [1, 37, 77, 64]           --\n",
       "│    │    └─Linear: 3-39                                [1, 19, 39, 256]          32,768\n",
       "│    │    └─LayerNorm: 3-40                             [1, 19, 39, 256]          512\n",
       "│    └─SwinResidualCrossAttention: 2-17                 [1, 37, 77, 64]           --\n",
       "│    │    └─MultiheadAttention: 3-41                    [200, 16, 64]             16,640\n",
       "│    │    └─LayerNorm: 3-42                             [1, 37, 77, 64]           128\n",
       "│    └─Sequential: 2-18                                 [1, 37, 77, 64]           --\n",
       "│    │    └─SwinTransformerBlockV2: 3-43                [1, 37, 77, 128]          203,912\n",
       "│    │    └─PointwiseConvolution: 3-44                  [1, 37, 77, 64]           8,256\n",
       "├─UnPatching: 1-6                                       [1, 64, 151, 309]         --\n",
       "│    └─Sequential: 2-19                                 [1, 64, 148, 308]         --\n",
       "│    │    └─Permute: 3-45                               [1, 64, 37, 77]           --\n",
       "│    │    └─ConvTranspose2d: 3-46                       [1, 64, 148, 308]         65,600\n",
       "│    │    └─BatchNorm2d: 3-47                           [1, 64, 148, 308]         128\n",
       "├─ConvolutionTriplet: 1-7                               [1, 64, 151, 309]         --\n",
       "│    └─Sequential: 2-20                                 [1, 64, 151, 309]         --\n",
       "│    │    └─Conv2d: 3-48                                [1, 64, 151, 309]         73,792\n",
       "│    │    └─BatchNorm2d: 3-49                           [1, 64, 151, 309]         128\n",
       "│    │    └─LeakyReLU: 3-50                             [1, 64, 151, 309]         --\n",
       "│    │    └─Conv2d: 3-51                                [1, 64, 151, 309]         36,928\n",
       "│    │    └─BatchNorm2d: 3-52                           [1, 64, 151, 309]         128\n",
       "│    │    └─LeakyReLU: 3-53                             [1, 64, 151, 309]         --\n",
       "│    │    └─Conv2d: 3-54                                [1, 64, 151, 309]         36,928\n",
       "│    │    └─BatchNorm2d: 3-55                           [1, 64, 151, 309]         128\n",
       "│    │    └─LeakyReLU: 3-56                             [1, 64, 151, 309]         --\n",
       "├─PointwiseConvolution: 1-8                             [1, 1, 151, 309]          --\n",
       "│    └─Conv2d: 2-21                                     [1, 1, 151, 309]          65\n",
       "=========================================================================================================\n",
       "Total params: 10,665,173\n",
       "Trainable params: 10,639,573\n",
       "Non-trainable params: 25,600\n",
       "Total mult-adds (G): 13.61\n",
       "=========================================================================================================\n",
       "Input size (MB): 0.56\n",
       "Forward/backward pass size (MB): 407.97\n",
       "Params size (MB): 30.07\n",
       "Estimated Total Size (MB): 438.60\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_size = [4, 4]\n",
    "embed_dim = 64\n",
    "depths = [1, 1, 1]\n",
    "num_heads = [4, 8, 16]\n",
    "window_size = [4, 4]\n",
    "num_classes = 1\n",
    "\n",
    "IMG_H, IMG_W = 151, 309\n",
    "\n",
    "global_stages = 1\n",
    "input_size = [IMG_H, IMG_W]\n",
    "final_downsample = True\n",
    "residual_cross_attention = True\n",
    "\n",
    "swin = XNetSwinTransformer(patch_size, embed_dim, depths, \n",
    "                           num_heads, window_size, num_classes=num_classes,\n",
    "                           global_stages=global_stages, input_size=input_size,\n",
    "                           final_downsample=final_downsample, residual_cross_attention=residual_cross_attention,\n",
    "                           )\n",
    "\n",
    "\n",
    "x = torch.randn((5, 3, IMG_H, IMG_W))\n",
    "print(x.shape)\n",
    "\n",
    "# print(swin)\n",
    "\n",
    "y = swin(x)\n",
    "print(y.shape)\n",
    "\n",
    "summary(swin, input_size=[1, 3, IMG_H, IMG_W])\n",
    "# 512 25\n",
    "# Parameter containing:\n",
    "# tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
    "#          [0., 0., 0.,  ..., 0., 0., 0.],\n",
    "#          [0., 0., 0.,  ..., 0., 0., 0.],\n",
    "#          ...,\n",
    "#          [0., 0., 0.,  ..., 0., 0., 0.],\n",
    "#          [0., 0., 0.,  ..., 0., 0., 0.],\n",
    "#          [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
    "# (625, 512)\n",
    "\n",
    "# 20 39\n",
    "# 10 19\n",
    "# 5 9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XNetSwinTransformer(\n",
      "  (smooth_conv_in): ConvolutionTriplet(\n",
      "    (layers): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): LeakyReLU(negative_slope=0.01)\n",
      "      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "  )\n",
      "  (patching): Patching(\n",
      "    (patching): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(4, 4), stride=(4, 4))\n",
      "      (1): Permute()\n",
      "      (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (encoder): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): SwinTransformerBlockV2(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftedWindowAttentionV2(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (cpb_mlp): Sequential(\n",
      "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=512, out_features=4, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=256, out_features=64, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): PatchMergingV2(\n",
      "      (reduction): Linear(in_features=256, out_features=128, bias=False)\n",
      "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): SwinTransformerBlockV2(\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftedWindowAttentionV2(\n",
      "          (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (cpb_mlp): Sequential(\n",
      "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=512, out_features=8, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.02, mode=row)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): PatchMergingV2(\n",
      "      (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): SwinTransformerBlockV2(\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftedWindowAttentionV2(\n",
      "          (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
      "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (cpb_mlp): Sequential(\n",
      "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=512, out_features=16, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.04, mode=row)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): PatchMergingV2(\n",
      "      (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (middle): Sequential(\n",
      "    (0): ViTEncoderBlock(\n",
      "      (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): ModuleList(\n",
      "    (0): PatchExpandingV2(\n",
      "      (expansion): Linear(in_features=512, out_features=1024, bias=False)\n",
      "      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): SwinResidualCrossAttention(\n",
      "      (cross_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): SwinTransformerBlockV2(\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftedWindowAttentionV2(\n",
      "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (cpb_mlp): Sequential(\n",
      "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=512, out_features=32, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.06000000000000001, mode=row)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): PointwiseConvolution(\n",
      "        (pointwise): Linear(in_features=512, out_features=256, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (3): PatchExpandingV2(\n",
      "      (expansion): Linear(in_features=256, out_features=512, bias=False)\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (4): SwinResidualCrossAttention(\n",
      "      (cross_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): SwinTransformerBlockV2(\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftedWindowAttentionV2(\n",
      "          (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
      "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (cpb_mlp): Sequential(\n",
      "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=512, out_features=16, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.08, mode=row)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): PointwiseConvolution(\n",
      "        (pointwise): Linear(in_features=256, out_features=128, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (6): PatchExpandingV2(\n",
      "      (expansion): Linear(in_features=128, out_features=256, bias=False)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (7): SwinResidualCrossAttention(\n",
      "      (cross_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (8): Sequential(\n",
      "      (0): SwinTransformerBlockV2(\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftedWindowAttentionV2(\n",
      "          (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (cpb_mlp): Sequential(\n",
      "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=512, out_features=8, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): PointwiseConvolution(\n",
      "        (pointwise): Linear(in_features=128, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (unpatching): UnPatching(\n",
      "    (unpatching): Sequential(\n",
      "      (0): Permute()\n",
      "      (1): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(4, 4))\n",
      "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (smooth_conv_out): ConvolutionTriplet(\n",
      "    (layers): Sequential(\n",
      "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): LeakyReLU(negative_slope=0.01)\n",
      "      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "  )\n",
      "  (head): PointwiseConvolution(\n",
      "    (pointwise): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(swin)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
