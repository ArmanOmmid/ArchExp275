{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from models.xswin import XNetSwinTransformer\n",
    "from models.modules import SwinResidualCrossAttention\n",
    "from models.modules.cross_attention_residual import _extract_windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "*A, B= 1, 2\n",
    "A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 8, 5])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([\n",
    "    [1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4],\n",
    "    [1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4],\n",
    "    [1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4],\n",
    "    [1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4],\n",
    "])\n",
    "\n",
    "x = torch.tensor([\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "])\n",
    "\n",
    "# print(torch.cat((x, x), dim=-1))\n",
    "\n",
    "\n",
    "# x = torch.vstack((torch.hstack((x, x*2)), torch.hstack((x*3, x*4))))\n",
    "x = torch.stack((x, x*2, x*3, x*4))\n",
    "x = torch.vstack((torch.hstack((x, x*2)), torch.hstack((x*3, x*4))))\n",
    "\n",
    "# print(x)\n",
    "\n",
    "# x = x.reshape(2, 2, -1)\n",
    "x = x.unsqueeze(0)\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "x = _extract_windows(x, 4, 4)\n",
    "\n",
    "# print(x.shape)\n",
    "\n",
    "# print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = torch.randn((5, 10, 20, 64))\n",
    "r2 = torch.randn((5, 10, 20, 64))\n",
    "\n",
    "SRCA = SwinResidualCrossAttention(\n",
    "    [4, 4],\n",
    "    64,\n",
    "    4,\n",
    ")\n",
    "\n",
    "# SRCA(r1, r2).shape\n",
    "\n",
    "# unfold = torch.nn.Unfold((4,4), dilation=1, padding=0, stride=(4,4))\n",
    "\n",
    "# r1 = r1.permute(0, -1, -3, -2)\n",
    "# print(r1.shape)\n",
    "# unfold(r1).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 160, 320])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/armanommid/Code/CSE/CSE275/ArchExp/ArchExp275/testing.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/armanommid/Code/CSE/CSE275/ArchExp/ArchExp275/testing.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/armanommid/Code/CSE/CSE275/ArchExp/ArchExp275/testing.ipynb#W3sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# print(swin)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/armanommid/Code/CSE/CSE275/ArchExp/ArchExp275/testing.ipynb#W3sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m y \u001b[39m=\u001b[39m swin(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/armanommid/Code/CSE/CSE275/ArchExp/ArchExp275/testing.ipynb#W3sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mprint\u001b[39m(y\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/armanommid/Code/CSE/CSE275/ArchExp/ArchExp275/testing.ipynb#W3sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mprint\u001b[39m(swin)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Code/CSE/CSE275/ArchExp/ArchExp275/models/xswin.py:230\u001b[0m, in \u001b[0;36mXNetSwinTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    227\u001b[0m residual \u001b[39m=\u001b[39m residuals[i_residual]\n\u001b[1;32m    229\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresidual_cross_attention:\n\u001b[0;32m--> 230\u001b[0m     residual \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder[i\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m](x, residual) \u001b[39m# Cross Attention Skip Connection\u001b[39;00m\n\u001b[1;32m    232\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((x, residual), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m# Dumb Skip Connection\u001b[39;00m\n\u001b[1;32m    234\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder[i\u001b[39m+\u001b[39m(\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresidual_cross_attention))](x)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Code/CSE/CSE275/ArchExp/ArchExp275/models/modules/cross_attention_residual.py:120\u001b[0m, in \u001b[0;36mSwinResidualCrossAttention.forward\u001b[0;34m(self, x, residual)\u001b[0m\n\u001b[1;32m    117\u001b[0m output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mreshape(\u001b[39m*\u001b[39mB, H \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_height, W \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_width, C)\n\u001b[1;32m    119\u001b[0m \u001b[39m# Unpadding\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m output \u001b[39m=\u001b[39m _fold_unpadding_prep(output, padding_info)\n\u001b[1;32m    122\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/Code/CSE/CSE275/ArchExp/ArchExp275/models/modules/cross_attention_residual.py:28\u001b[0m, in \u001b[0;36m_fold_unpadding_prep\u001b[0;34m(x, padding_info)\u001b[0m\n\u001b[1;32m     25\u001b[0m pad_height, pad_width \u001b[39m=\u001b[39m padding_info\n\u001b[1;32m     27\u001b[0m \u001b[39mif\u001b[39;00m pad_height \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 28\u001b[0m     x \u001b[39m=\u001b[39m x[:, x:\u001b[39m-\u001b[39;49mx, :, :]\n\u001b[1;32m     29\u001b[0m \u001b[39mif\u001b[39;00m pad_width \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     30\u001b[0m     x \u001b[39m=\u001b[39m x[:, :, x:\u001b[39m-\u001b[39mx, :]\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "patch_size = [4, 4]\n",
    "embed_dim = 64\n",
    "depths = [2, 2, 2]\n",
    "num_heads = [4, 8, 16]\n",
    "window_size = [4, 4]\n",
    "num_classes = 1\n",
    "\n",
    "\n",
    "swin = XNetSwinTransformer(patch_size, embed_dim, depths, \n",
    "                           num_heads, window_size, num_classes=num_classes,\n",
    "                           final_downsample=False, residual_cross_attention=True,\n",
    "                           )\n",
    "\n",
    "IMG = 160\n",
    "x = torch.randn((5, 3, IMG, IMG*2))\n",
    "print(x.shape)\n",
    "\n",
    "# print(swin)\n",
    "\n",
    "y = swin(x)\n",
    "print(y.shape)\n",
    "\n",
    "print(swin)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
