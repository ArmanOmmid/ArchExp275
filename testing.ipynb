{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from models.xswin import XNetSwinTransformer\n",
    "from models.modules import SwinResidualCrossAttention\n",
    "\n",
    "from models.modules.swin_residual_cross_attention import _extract_windows, _unfold_padding_prep, _fold_unpadding_prep\n",
    "\n",
    "from torchinfo import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "*A, B= 1, 2\n",
    "A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 8, 5])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([\n",
    "    [1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4],\n",
    "    [1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4],\n",
    "    [1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4],\n",
    "    [1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4],\n",
    "])\n",
    "\n",
    "x = torch.tensor([\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "])\n",
    "\n",
    "# print(torch.cat((x, x), dim=-1))\n",
    "\n",
    "\n",
    "# x = torch.vstack((torch.hstack((x, x*2)), torch.hstack((x*3, x*4))))\n",
    "x = torch.stack((x, x*2, x*3, x*4))\n",
    "x = torch.vstack((torch.hstack((x, x*2)), torch.hstack((x*3, x*4))))\n",
    "\n",
    "\n",
    "# x = x.reshape(2, 2, -1)\n",
    "x = x.unsqueeze(0)\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "x = _extract_windows(x, 4, 4)\n",
    "\n",
    "# print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([], size=(12, 0))\n",
      "torch.Size([5, 10, 22, 16])\n",
      "IN\n",
      "torch.Size([5, 10, 22, 16]) torch.Size([5, 10, 22, 16])\n",
      "OUT\n",
      "torch.Size([5, 3, 6, 16, 16]) torch.Size([5, 3, 6, 16, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 22, 16])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_DIM = 16\n",
    "\n",
    "r1 = torch.ones((5, 10, 22, H_DIM))\n",
    "r2 = torch.ones(r1.shape)\n",
    "\n",
    "f1, pinfo = _unfold_padding_prep(r1, 4, 4)\n",
    "print(f1[0, :, :0, 0])\n",
    "f2 = _fold_unpadding_prep(f1, pinfo)\n",
    "print(f2.shape)\n",
    "\n",
    "SRCA = SwinResidualCrossAttention(\n",
    "    [4, 4],\n",
    "    H_DIM,\n",
    "    4,\n",
    ")\n",
    "\n",
    "SRCA(r1, r2).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 164, 328])\n",
      "torch.Size([5, 41, 82, 64])\n",
      "E torch.Size([5, 41, 82, 64])\n",
      "Down torch.Size([5, 21, 41, 128])\n",
      "E torch.Size([5, 21, 41, 128])\n",
      "Down torch.Size([5, 11, 21, 256])\n",
      "E torch.Size([5, 11, 21, 256])\n",
      "IN\n",
      "torch.Size([5, 11, 21, 256]) torch.Size([5, 11, 21, 256])\n",
      "OUT\n",
      "torch.Size([5, 3, 6, 16, 256]) torch.Size([5, 3, 6, 16, 256])\n",
      "Up torch.Size([5, 22, 42, 128])\n",
      "IN\n",
      "torch.Size([5, 22, 42, 128]) torch.Size([5, 21, 41, 128])\n",
      "OUT\n",
      "torch.Size([5, 6, 11, 16, 128]) torch.Size([5, 6, 11, 16, 128])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 3. Expected size 22 but got size 21 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/armanommid/Code/CSE/CSE275/ArchExp/ArchExp275/testing.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/armanommid/Code/CSE/CSE275/ArchExp/ArchExp275/testing.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/armanommid/Code/CSE/CSE275/ArchExp/ArchExp275/testing.ipynb#W3sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# print(swin)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/armanommid/Code/CSE/CSE275/ArchExp/ArchExp275/testing.ipynb#W3sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m y \u001b[39m=\u001b[39m swin(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/armanommid/Code/CSE/CSE275/ArchExp/ArchExp275/testing.ipynb#W3sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mprint\u001b[39m(y\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/armanommid/Code/CSE/CSE275/ArchExp/ArchExp275/testing.ipynb#W3sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# print(swin)\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Code/CSE/CSE275/ArchExp/ArchExp275/models/xswin.py:238\u001b[0m, in \u001b[0;36mXNetSwinTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresidual_cross_attention:\n\u001b[1;32m    236\u001b[0m         residual \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder[i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m](x, residual) \u001b[39m# Cross Attention Skip Connection\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat((x, residual), dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m) \u001b[39m# Dumb Skip Connection\u001b[39;00m\n\u001b[1;32m    240\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder[i\u001b[39m+\u001b[39m(\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresidual_cross_attention))](x)\n\u001b[1;32m    242\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munpatching(x)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 3. Expected size 22 but got size 21 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "patch_size = [4, 4]\n",
    "embed_dim = 64\n",
    "depths = [3, 3, 3]\n",
    "num_heads = [4, 8, 16]\n",
    "window_size = [4, 4]\n",
    "num_classes = 1\n",
    "\n",
    "\n",
    "swin = XNetSwinTransformer(patch_size, embed_dim, depths, \n",
    "                           num_heads, window_size, num_classes=num_classes,\n",
    "                           final_downsample=False, residual_cross_attention=True,\n",
    "                           )\n",
    "\n",
    "IMG_H, IMG_W = 164, 328\n",
    "x = torch.randn((5, 3, IMG_H, IMG_W))\n",
    "print(x.shape)\n",
    "\n",
    "# print(swin)\n",
    "\n",
    "y = swin(x)\n",
    "print(y.shape)\n",
    "\n",
    "# print(swin)\n",
    "\n",
    "summary(swin, input_size=[1, 3, IMG_H, IMG_W])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
