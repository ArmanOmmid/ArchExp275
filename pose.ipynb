{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import kornia.geometry.conversions as conversions\n",
    "\n",
    "\n",
    "from torchinfo import summary\n",
    "from models import XSwinFusion \n",
    "from pose_estimation import PoseDataNPZTorch, PoseDataNPZ, PoseData, icp, PoseDataNPZSegmentationTorch\n",
    "from pose_estimation.utils import crop_and_resize_multiple, enumerate_symmetries, COLOR_PALETTE\n",
    "\n",
    "WORKDIR = f\"{os.getcwd()}/..\"\n",
    "DATA_FOLDER = os.path.join(WORKDIR, \"data_folder\")\n",
    "DATASET_NPZ_PATH = os.path.join(DATA_FOLDER, \"dataset_npz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presumed Preloaded NPZ Dataset: /Users/armanommid/Code/CSE/CSE275/HW2/XSwinDiffusion/../data_folder/dataset_npz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2.27220729, 3.85512333, 0.06790825, 2.42104538, 2.07067204,\n",
       "       3.93925143, 0.64182094, 1.18777512, 0.4407341 , 0.52512214,\n",
       "       0.34137755, 1.86846364, 3.81615646, 3.64147644, 0.31318209,\n",
       "       2.69653439, 1.63886531, 3.56737491, 3.64305267, 1.47233098,\n",
       "       2.76495753, 3.64190419, 0.60392237, 1.31382749, 3.93023501,\n",
       "       1.97133623, 2.24107948, 3.32767968, 1.14904461, 0.33874604,\n",
       "       2.3076569 , 1.05357246, 0.52383978, 0.93881299, 0.87111473,\n",
       "       2.13725172, 3.43294173, 0.92985386, 3.95793929, 0.51232972,\n",
       "       2.475748  , 0.79718815, 0.68471138, 0.85126059, 0.15536348,\n",
       "       1.9916658 , 0.13429935, 0.42351451, 1.27801061, 0.39196135,\n",
       "       1.04016916, 1.51138543, 2.26624893, 1.88724767, 2.24454911,\n",
       "       0.92508584, 1.31921162, 2.03160651, 0.45497933, 0.84568012,\n",
       "       1.15797339, 0.93588777, 1.27437227, 1.66202315, 1.79084296,\n",
       "       1.43061129, 3.72109191, 4.05522843, 4.00312151, 1.03946704,\n",
       "       1.45463902, 0.95911586, 0.71062844, 0.96072785, 0.61686594,\n",
       "       1.21984134, 2.23276583, 1.7477317 , 4.02962064, 0.1       ,\n",
       "       0.1       , 0.1       ])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = PoseDataNPZSegmentationTorch(DATASET_NPZ_PATH, resize=(144, 256))\n",
    "dataloader = DataLoader(data, batch_size=64, shuffle=True)\n",
    "# for i, (im, l, k) in enumerate(dataloader):\n",
    "#     print(torch.min(l))\n",
    "\n",
    "volumes = np.zeros(len(data.data.info) + 3)\n",
    "for i, info in enumerate(data.data.info):\n",
    "    volumes[i] = info[\"width\"] * info[\"height\"] * info[\"length\"]\n",
    "normalized_volumes = volumes / volumes.mean()\n",
    "normalized_volumes += 0.2\n",
    "normalized_volumes **= -1\n",
    "normalized_volumes[-3:] = 0.1\n",
    "normalized_volumes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOWCASE = False\n",
    "if SHOWCASE:\n",
    "\n",
    "    data = PoseDataNPZ(DATASET_NPZ_PATH)\n",
    "    scene = data[1, 1, 4]\n",
    "    rgb = scene[\"color\"]\n",
    "    depth = scene[\"depth\"]\n",
    "    label = scene[\"label\"]\n",
    "    meta = scene[\"meta\"]\n",
    "\n",
    "    mask = label == np.unique(label)[0]\n",
    "    target_size = (432, 768)\n",
    "    margin = 8\n",
    "    aspect_ratio = True\n",
    "    mask_fill = False\n",
    "\n",
    "    (rgb_cr, depth_cr, label_cr, mask_cr), scale, translate = crop_and_resize_multiple(\n",
    "        (rgb, depth, COLOR_PALETTE[label], mask), mask, target_size=target_size, margin=margin, \n",
    "        aspect_ratio=aspect_ratio, mask_fill=mask_fill)\n",
    "\n",
    "\n",
    "    # print(depth[mask][:200])\n",
    "    # print(depth_cr[mask_cr][:200])\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(rgb_cr)\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(depth_cr)\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(label_cr)  # draw colorful segmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FCN_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=FCN_ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presumed Preloaded NPZ Dataset: /Users/armanommid/Code/CSE/CSE275/HW2/XSwinDiffusion/../data_folder/dataset_npz\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "samples = 1_000\n",
    "resize = (144, 256)\n",
    "aspect_ratio = True\n",
    "margin = 12\n",
    "feature_dims = 64\n",
    "quaternion = True\n",
    "pretrained_resnet = False\n",
    "\n",
    "\n",
    "model = XSwinFusion(feature_dims=feature_dims, resize=resize, \n",
    "                    quaternion=quaternion, pretrained=pretrained_resnet)\n",
    "\n",
    "dataset = None\n",
    "dataset = PoseDataNPZTorch(DATASET_NPZ_PATH, samples=samples, \n",
    "                           resize=resize, aspect_ratio=aspect_ratio, \n",
    "                           margin=margin, symmetry_pad=64)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# summary(model, depth=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2, 97, 19])\n",
      "tensor([ 1, 25,  2])\n",
      "tensor([  3, 158,   8])\n",
      "tensor([ 2, 92,  1])\n",
      "tensor([ 1, 49, 19])\n",
      "tensor([ 1,  9, 15])\n",
      "tensor([ 2, 82, 39])\n",
      "tensor([ 2, 47, 13])\n",
      "tensor([ 1, 44,  3])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/armanommid/Code/CSE/CSE275/HW2/XSwinDiffusion/pose.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/armanommid/Code/CSE/CSE275/HW2/XSwinDiffusion/pose.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m points \u001b[39m=\u001b[39m []\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/armanommid/Code/CSE/CSE275/HW2/XSwinDiffusion/pose.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, (s, t, c, mi, p, sym, key) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/armanommid/Code/CSE/CSE275/HW2/XSwinDiffusion/pose.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m(key[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Code/CSE/CSE275/HW2/XSwinDiffusion/pose_estimation/pose_data_npz.py:219\u001b[0m, in \u001b[0;36mPoseDataNPZTorch.__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    215\u001b[0m     pose \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    217\u001b[0m sym \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39minfo[obj_id][\u001b[39m\"\u001b[39m\u001b[39mgeometric_symmetry\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m--> 219\u001b[0m source_pcd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample_source_pcd(obj_id) \u001b[39m*\u001b[39m meta[\u001b[39m\"\u001b[39m\u001b[39mscales\u001b[39m\u001b[39m\"\u001b[39m][obj_id]\n\u001b[1;32m    221\u001b[0m sym \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_symmetry(obj_id)\n\u001b[1;32m    223\u001b[0m \u001b[39mreturn\u001b[39;00m source_pcd, target_pcd, color, mask_indices, pose, sym, np\u001b[39m.\u001b[39marray(key)\n",
      "File \u001b[0;32m~/Code/CSE/CSE275/HW2/XSwinDiffusion/pose_estimation/pose_data_npz.py:163\u001b[0m, in \u001b[0;36mPoseDataNPZTorch.sample_source_pcd\u001b[0;34m(self, obj_id)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msample_source_pcd\u001b[39m(\u001b[39mself\u001b[39m, obj_id):\n\u001b[1;32m    161\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msource_pcd_cache[obj_id] \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msource_pcd_cache[obj_id] \u001b[39m=\u001b[39m \\\n\u001b[0;32m--> 163\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49msample_mesh(obj_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msamples)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m    165\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msource_pcd_cache[obj_id]\n",
      "File \u001b[0;32m~/Code/CSE/CSE275/HW2/XSwinDiffusion/pose_estimation/pose_data_npz.py:108\u001b[0m, in \u001b[0;36mPoseDataNPZ.sample_mesh\u001b[0;34m(self, obj_id, n)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msample_mesh\u001b[39m(\u001b[39mself\u001b[39m, obj_id, n):\n\u001b[0;32m--> 108\u001b[0m     \u001b[39mreturn\u001b[39;00m trimesh\u001b[39m.\u001b[39msample\u001b[39m.\u001b[39msample_surface(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_mesh(obj_id), n)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Code/CSE/CSE275/HW2/XSwinDiffusion/pose_estimation/pose_data_npz.py:95\u001b[0m, in \u001b[0;36mPoseDataNPZ.get_mesh\u001b[0;34m(self, obj_id)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobject_RAM_cache[obj_id]\n\u001b[1;32m     94\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjects, np\u001b[39m.\u001b[39mlib\u001b[39m.\u001b[39mnpyio\u001b[39m.\u001b[39mNpzFile):\n\u001b[0;32m---> 95\u001b[0m     mesh \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobjects[\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mobj_id\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     96\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjects \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     97\u001b[0m     mesh \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpose_data\u001b[39m.\u001b[39mget_mesh(obj_id)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/lib/npyio.py:245\u001b[0m, in \u001b[0;36mNpzFile.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[39mif\u001b[39;00m magic \u001b[39m==\u001b[39m \u001b[39mformat\u001b[39m\u001b[39m.\u001b[39mMAGIC_PREFIX:\n\u001b[1;32m    244\u001b[0m     \u001b[39mbytes\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mzip\u001b[39m.\u001b[39mopen(key)\n\u001b[0;32m--> 245\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mformat\u001b[39;49m\u001b[39m.\u001b[39;49mread_array(\u001b[39mbytes\u001b[39;49m,\n\u001b[1;32m    246\u001b[0m                              allow_pickle\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mallow_pickle,\n\u001b[1;32m    247\u001b[0m                              pickle_kwargs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpickle_kwargs)\n\u001b[1;32m    248\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mzip\u001b[39m.\u001b[39mread(key)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/lib/format.py:746\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    744\u001b[0m     pickle_kwargs \u001b[39m=\u001b[39m {}\n\u001b[1;32m    745\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 746\u001b[0m     array \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39;49mload(fp, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_kwargs)\n\u001b[1;32m    747\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mUnicodeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    748\u001b[0m     \u001b[39m# Friendlier error message\u001b[39;00m\n\u001b[1;32m    749\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mUnicodeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnpickling a python object failed: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    750\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mYou may need to pass the encoding= option \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    751\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mto numpy.load\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (err,)) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/zipfile.py:928\u001b[0m, in \u001b[0;36mZipExtFile.read\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    926\u001b[0m         buf \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m data[:n]\n\u001b[1;32m    927\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 928\u001b[0m     buf \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m data\n\u001b[1;32m    929\u001b[0m     n \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(data)\n\u001b[1;32m    930\u001b[0m \u001b[39mreturn\u001b[39;00m buf\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for i, (s, t, c, mi, p, sym, key, obj_id) in enumerate(dataloader):\n",
    "    print(key[0])\n",
    "\n",
    "    if i == 0:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = dataloader.dataset.data.info\n",
    "for obj in objects:\n",
    "    pass\n",
    "\n",
    "sym_pad = torch.eye(3).unsqueeze(0).repeat(64, 1, 1)\n",
    "s = enumerate_symmetries(objects[5][\"geometric_symmetry\"])\n",
    "s = torch.cat(s)\n",
    "print(len(s))\n",
    "sym_pad[:len(s), :, :] = s\n",
    "sym_pad\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
